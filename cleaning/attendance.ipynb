{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24dfb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a39bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 done\n",
      "19 done\n",
      "File /Users/danielmatten/desktop/m/attendance_pub2020.sas7bdat not found. Skipping.\n",
      "21 done\n",
      "22 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "schl_yr\n",
      "dayp\n",
      "daya\n",
      "daye\n",
      "lea\n",
      "schlcode\n",
      "mastid\n",
      "grade\n",
      "daysabs\n",
      "daysmem\n"
     ]
    }
   ],
   "source": [
    "attendance = pd.DataFrame()\n",
    "for year in range(18, 23):  # range is exclusive at the end\n",
    "    filename = f\"/Users/danielmatten/desktop/m/attendance_pub20{year}.sas7bdat\"\n",
    "    try:\n",
    "        # Read SAS file\n",
    "        df = pd.read_sas(filename)\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # Drop missing values\n",
    "\n",
    "        \n",
    "        # Append to the main DataFrame\n",
    "        attendance = pd.concat([attendance, df], ignore_index=True)\n",
    "        print(f\"{year} done\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping.\")\n",
    "    except KeyError:\n",
    "        print(f\"Required columns not found in {filename}. Skipping.\")\n",
    "        \n",
    "for year in range(14,18):\n",
    "    filename = f\"/Users/danielmatten/desktop/m/pcaudit_pub20{year}.sas7bdat\"\n",
    "    try:\n",
    "        # Read SAS file\n",
    "        df = pd.read_sas(filename)\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # Drop missing values\n",
    "        columns_to_keep = ['mastid', 'grade', 'daysabs', 'daysmem']\n",
    "\n",
    "        # Filter the DataFrame to keep only the specified columns\n",
    "        df = df[columns_to_keep]\n",
    "        \n",
    "        # Append to the main DataFrame\n",
    "        attendance = pd.concat([attendance, df], ignore_index=True)\n",
    "        print(f\"{year} done\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping.\")\n",
    "    except KeyError:\n",
    "        print(f\"Required columns not found in {filename}. Skipping.\")\n",
    "for year in range(10,14):\n",
    "    filename = f\"/Users/danielmatten/desktop/m/accdemopub20{year}.sas7bdat\"\n",
    "    try:\n",
    "        # Read SAS file\n",
    "        df = pd.read_sas(filename)\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # Drop missing values\n",
    "\n",
    "        columns_to_keep = ['mastid', 'grade', 'daysabs', 'daysmem']\n",
    "\n",
    "        # Filter the DataFrame to keep only the specified columns\n",
    "        df = df[columns_to_keep]\n",
    "        # Append to the main DataFrame\n",
    "        attendance = pd.concat([attendance, df], ignore_index=True)\n",
    "        print(f\"{year} done\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping.\")\n",
    "    except KeyError:\n",
    "        print(f\"Required columns not found in {filename}. Skipping.\")\n",
    "\n",
    "\n",
    "def clean_bytes(val):\n",
    "    if isinstance(val, bytes):\n",
    "        return str(val)[2:-1]  # str(b'xyz') => \"b'xyz'\" → \"xyz\"\n",
    "    return val\n",
    "for col in attendance.columns:\n",
    "    attendance[col] = attendance[col].apply(clean_bytes)\n",
    "    print(col)\n",
    "    \n",
    "#attendance['schl_yr'] = attendance['schl_yr'].str.split('-').str[1].astype(int)\n",
    "#attendance.dropna(subset=['mastid'], inplace=True)\n",
    "#attendance.to_csv(\"/Users/adamcartwright/NCERDC-MODEL/data/attendance_master.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance.to_csv(\"../data/attendance_master.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d74b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_ids = pd.read_csv('../data/exit_list.csv')\n",
    "# If squeeze doesn’t work (Pandas 2.x), just:\n",
    "# Then use to filter another DataFrame\n",
    "filtered = attendance.merge(exit_ids,on='mastid',how='inner')\n",
    "filtered = filtered[filtered['mastid'].isin(exit_ids['mastid'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.drop(columns=['lea','schlcode','dayp'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['daya'] = filtered['daya'].fillna(filtered['daysabs'])\n",
    "\n",
    "# Fill NaN values in 'daye' with the values from 'daysmem'\n",
    "filtered['daye'] = filtered['daye'].fillna(filtered['daysmem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.drop(columns=['daysmem','daysabs'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['grade'] = pd.to_numeric(filtered['grade'], errors='coerce')\n",
    "\n",
    "# Step 2: Fill missing 'grade' values with the calculated formula\n",
    "# Extract the year after the '-' in 'schlyear' and convert to numeric\n",
    "filtered['schlyear_year'] = filtered['schl_yr'].str.split('-').str[0].astype(float)\n",
    "\n",
    "# Calculate the missing grade values using the formula\n",
    "filtered['grade'] = filtered['grade'].fillna(\n",
    "    filtered['schlyear_year'] - filtered['effective_g9year'] + 9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.drop(columns=['schl_yr','effective_g9year','schlyear_year'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d2456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure daya and daye are numeric\n",
    "filtered['daya'] = pd.to_numeric(filtered['daya'], errors='coerce')\n",
    "filtered['daye'] = pd.to_numeric(filtered['daye'], errors='coerce')\n",
    "\n",
    "# Compute absent percent\n",
    "filtered['absent_pc'] = filtered['daya'] / filtered['daye']\n",
    "\n",
    "# Filter to grades 6 through 9 only\n",
    "grade_range = [3,4,5,6,7,8,9,10,11]\n",
    "filtered = filtered[filtered['grade'].isin(grade_range)]\n",
    "\n",
    "# Count how many distinct grades each student has in that range\n",
    "grade_counts = filtered.groupby('mastid')['grade'].nunique()\n",
    "\n",
    "\n",
    "# Group by mastid and grade and take mean absent_pc\n",
    "agg_df = filtered.groupby(['mastid', 'grade'])['absent_pc'].mean().reset_index()\n",
    "\n",
    "# Now pivot safely\n",
    "pivot_df = agg_df.pivot(index='mastid', columns='grade', values='absent_pc')\n",
    "pivot_df.columns = [f'absent_pc_grade_{g}' for g in pivot_df.columns]\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c27ca7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip '.0' from all column names\n",
    "pivot_df.columns = [\n",
    "    col.replace('.0', '') if isinstance(col, str) else col\n",
    "    for col in pivot_df.columns\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbef5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute grade 7 if missing\n",
    "pivot_df['absent_pc_grade_7'] = pivot_df['absent_pc_grade_7'].fillna(\n",
    "    (pivot_df['absent_pc_grade_6'] + pivot_df['absent_pc_grade_8']) / 2\n",
    ")\n",
    "\n",
    "# Impute grade 8 if missing\n",
    "pivot_df['absent_pc_grade_8'] = pivot_df['absent_pc_grade_8'].fillna(\n",
    "    (pivot_df['absent_pc_grade_7'] + pivot_df['absent_pc_grade_9']) / 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     69730\n",
      "1    295742\n",
      "2     68923\n",
      "3     22065\n",
      "4     15150\n",
      "5     15909\n",
      "6     22236\n",
      "7      1683\n",
      "8         9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'pivot_df' is your DataFrame\n",
    "\n",
    "# Count the number of NaN values per row\n",
    "na_counts_per_row = pivot_df.isna().sum(axis=1)\n",
    "\n",
    "# Create a summary table of the NaN counts per row\n",
    "na_counts_per_row_summary = na_counts_per_row.value_counts().sort_index()\n",
    "\n",
    "# Display the result\n",
    "print(na_counts_per_row_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'pivot_df' is your DataFrame\n",
    "\n",
    "# Step 1: Filter rows with 1 or fewer NaNs\n",
    "filtered_df = pivot_df[pivot_df.isna().sum(axis=1) <= 1]\n",
    "\n",
    "# Step 2: Impute missing values based on the conditions\n",
    "def impute_missing_values(row):\n",
    "    # If there is exactly one NaN in the row, find its position\n",
    "    if row.isna().sum() == 1:\n",
    "        # Check if NaN is in 'absent_pc_grade_3' or 'absent_pc_grade_11'\n",
    "        nan_column = row[row.isna()].index[0]\n",
    "        \n",
    "        if nan_column not in ['absent_pc_grade_3', 'absent_pc_grade_11']:\n",
    "            # Find the previous and next columns to impute from\n",
    "            grade_number = int(nan_column.split('_')[-1])  # Get the grade number (e.g., 3 or 11)\n",
    "            prev_column = f\"absent_pc_grade_{grade_number - 1}\"\n",
    "            next_column = f\"absent_pc_grade_{grade_number + 1}\"\n",
    "\n",
    "            # Check if both previous and next columns exist\n",
    "            if prev_column in row.index and next_column in row.index:\n",
    "                # Impute as the mean of the previous and next grade\n",
    "                row[nan_column] = np.mean([row[prev_column], row[next_column]])\n",
    "\n",
    "    return row\n",
    "\n",
    "# Apply the imputation function to each row\n",
    "filtered_df = filtered_df.apply(impute_missing_values, axis=1)\n",
    "\n",
    "# Display the updated DataFrame (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "687ed717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.to_csv(\"../data/attendance_mastid.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
