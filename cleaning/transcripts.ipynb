{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/Users/danielmatten/Desktop/m/transcripts20\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.sas7bdat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Read SAS file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Keep only relevant columns\u001b[39;00m\n\u001b[32m     14\u001b[39m     df.columns = df.columns.str.lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/sas/sasreader.py:178\u001b[39m, in \u001b[36mread_sas\u001b[39m\u001b[34m(filepath_or_buffer, format, index, encoding, chunksize, iterator, compression)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reader\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m reader:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/sas/sas7bdat.py:685\u001b[39m, in \u001b[36mSAS7BDATReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28mself\u001b[39m._current_row_in_chunk_index = \u001b[32m0\u001b[39m\n\u001b[32m    684\u001b[39m p = Parser(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m rslt = \u001b[38;5;28mself\u001b[39m._chunk_to_dataframe()\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32msas.pyx:382\u001b[39m, in \u001b[36mpandas._libs.sas.Parser.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msas.pyx:473\u001b[39m, in \u001b[36mpandas._libs.sas.Parser.readline\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msas.pyx:394\u001b[39m, in \u001b[36mpandas._libs.sas.Parser.read_next_page\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/sas/sas7bdat.py:693\u001b[39m, in \u001b[36mSAS7BDATReader._read_next_page\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    689\u001b[39m         rslt = rslt.set_index(\u001b[38;5;28mself\u001b[39m.index)\n\u001b[32m    691\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rslt\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_next_page\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    694\u001b[39m     \u001b[38;5;28mself\u001b[39m._current_page_data_subheader_pointers = []\n\u001b[32m    695\u001b[39m     \u001b[38;5;28mself\u001b[39m._cached_page = \u001b[38;5;28mself\u001b[39m._path_or_buf.read(\u001b[38;5;28mself\u001b[39m._page_length)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "transcripts = pd.DataFrame()\n",
    "for year in range(18, 24):  # range is exclusive at the end\n",
    "    filename = f\"/Users/danielmatten/Desktop/m/transcripts20{year}.sas7bdat\"\n",
    "    try:\n",
    "        # Read SAS file\n",
    "        df = pd.read_sas(filename)\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # Drop missing values\n",
    "\n",
    "        \n",
    "        # Append to the main DataFrame\n",
    "        transcripts = pd.concat([transcripts, df], ignore_index=True)\n",
    "        print(f\"{year} done\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping.\")\n",
    "    except KeyError:\n",
    "        print(f\"Required columns not found in {filename}. Skipping.\")\n",
    "def clean_bytes(val):\n",
    "    if isinstance(val, bytes):\n",
    "        return str(val)[2:-1]  # str(b'xyz') => \"b'xyz'\" → \"xyz\"\n",
    "    return val\n",
    "# Drop unnecessary columns\n",
    "transcripts.drop(columns=['semester', 'how_taken_desc', 'course_length_desc','school','completed_date','course_code'], inplace=True)\n",
    "\n",
    "# Drop rows where 'mastid' is NaN\n",
    "#transcripts.dropna(subset=['mastid'], inplace=True)\n",
    "print(\"nas\")\n",
    "# Decode bytes to string in all columns (no slicing needed)\n",
    "for col in transcripts.columns:\n",
    "    transcripts[col] = transcripts[col].apply(clean_bytes)\n",
    "    print(col)\n",
    "\n",
    "transcripts.dropna(inplace=True) #this is pretty safe\n",
    "\n",
    "transcripts.to_csv(\"../data/transcripts_master.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── GPA HELPER FUNCTIONS ───────────────────────────────────────────────────────\n",
    "\n",
    "def final_mark_to_letter(mark):\n",
    "    \"\"\"\n",
    "    Convert a final_mark (numeric 0–100 or a letter) into a standardized letter grade:\n",
    "      - If it can be cast to float, map 90–100→\"A\", 80–89→\"B\", 70–79→\"C\", 60–69→\"D\", <60→\"F\"\n",
    "      - Otherwise, assume it’s already a letter string (e.g. \"A\", \"B+\", \"C-\", etc.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = float(mark)\n",
    "        if score >= 90:\n",
    "            return \"A\"\n",
    "        elif score >= 80:\n",
    "            return \"B\"\n",
    "        elif score >= 70:\n",
    "            return \"C\"\n",
    "        elif score >= 60:\n",
    "            return \"D\"\n",
    "        else:\n",
    "            return \"F\"\n",
    "    except:\n",
    "        return str(mark).strip().upper()\n",
    "\n",
    "base_points_map = {\n",
    "    \"A+\": 4.0, \"A\": 4.0, \"A-\": 4.0,\n",
    "    \"B+\": 3.0, \"B\": 3.0, \"B-\": 3.0,\n",
    "    \"C+\": 2.0, \"C\": 2.0, \"C-\": 2.0,\n",
    "    \"D+\": 1.0, \"D\": 1.0, \"D-\": 1.0,\n",
    "    \"F\": 0.0\n",
    "}\n",
    "\n",
    "def extra_weight(level):\n",
    "    \"\"\"\n",
    "    Map academic_level_desc codes to extra GPA weight:\n",
    "      - 5 → Honors/Advanced/AIG   → +0.5\n",
    "      - 7 → Advanced Placement     → +1.0\n",
    "      - 8 → International Baccalaureate → +1.0\n",
    "      - Otherwise → 0.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lvl = int(level)\n",
    "        if lvl == 5:\n",
    "            return 0.5\n",
    "        elif lvl in (7, 8):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# ─── 1) Copy & normalize ────────────────────────────────────────────────────────\n",
    "df = transcripts.copy()\n",
    "# ensure datetime\n",
    "df[\"completed_date\"] = pd.to_datetime(df[\"completed_date\"], errors=\"coerce\")\n",
    "df[\"year\"]           = df[\"completed_date\"].dt.year.astype(\"Int64\")\n",
    "\n",
    "# ─── 2) Filter to HS grades & GPA-eligible ───────────────────────────────────────\n",
    "df[\"grade_int\"] = pd.to_numeric(df[\"grade\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df = df.loc[\n",
    "    df[\"grade_int\"].between(9, 12) &\n",
    "    (df[\"include_in_gpa\"] == \"Y\")\n",
    "].copy()\n",
    "\n",
    "# ─── 4) GPA points & quality points ─────────────────────────────────────────────\n",
    "# letter → base points → extra weight → capped weighted points\n",
    "df[\"letter_grade\"]         = df[\"final_mark\"].apply(final_mark_to_letter)\n",
    "df[\"base_grade_point\"]     = df[\"letter_grade\"].map(base_points_map)\n",
    "df[\"extra_gpa_weight\"]     = df[\"academic_level_desc\"].apply(extra_weight)\n",
    "df[\"weighted_grade_point\"] = (\n",
    "    df[\"base_grade_point\"] + df[\"extra_gpa_weight\"]\n",
    ").clip(upper=5.0)\n",
    "\n",
    "# credits\n",
    "df[\"credit_value_earned\"] = pd.to_numeric(df[\"credit_value_earned\"], errors=\"coerce\")\n",
    "df[\"credit_value\"]        = pd.to_numeric(df[\"credit_value\"], errors=\"coerce\")\n",
    "df[\"credits_for_calc\"]    = df[\"credit_value_earned\"].fillna(df[\"credit_value\"])\n",
    "\n",
    "# quality points\n",
    "df[\"qp_unweighted\"] = df[\"base_grade_point\"]     * df[\"credits_for_calc\"]\n",
    "df[\"qp_weighted\"]   = df[\"weighted_grade_point\"] * df[\"credits_for_calc\"]\n",
    "\n",
    "# ─── 5) Cumulative sums & GPAs ──────────────────────────────────────────────────\n",
    "df = df.sort_values([\"mastid\", \"year\"])\n",
    "df[[\"csum_credits\",\"csum_qp_uw\",\"csum_qp_w\"]] = (\n",
    "    df.groupby(\"mastid\")[[\"credits_for_calc\",\"qp_unweighted\",\"qp_weighted\"]]\n",
    "      .cumsum()\n",
    ")\n",
    "\n",
    "df[\"gpa_unweighted_cum\"] = (df[\"csum_qp_uw\"] / df[\"csum_credits\"]).round(3)\n",
    "df[\"gpa_weighted_cum\"]   = (df[\"csum_qp_w\"]  / df[\"csum_credits\"]).round(3)\n",
    "\n",
    "# ─── 6) Collapse to one row per student/school/year ──────────────────────────────\n",
    "gpa_yearly = (\n",
    "    df.groupby([\"mastid\",\"year\"], as_index=False)\n",
    "      .agg({\n",
    "         \"gpa_unweighted_cum\":\"last\",\n",
    "         \"gpa_weighted_cum\":\"last\",\n",
    "         \"csum_credits\":\"last\"\n",
    "      })\n",
    "      .rename(columns={\n",
    "         \"csum_credits\":\"total_credits_cum\"\n",
    "      })\n",
    ")\n",
    "\n",
    "# clean up types & drop empties\n",
    "gpa_yearly[\"mastid\"] = gpa_yearly[\"mastid\"].astype(int)\n",
    "gpa_yearly.dropna(subset=[\"gpa_unweighted_cum\"], inplace=True)\n",
    "\n",
    "# ─── 7) Save ────────────────────────────────────────────────────────────────────\n",
    "out_path = \"/Users/adamcartwright/ncerdc-model/data/transcripts_gpa_by_year.csv\"\n",
    "gpa_yearly.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping based on the examples you gave\n",
    "conversion_map = {\n",
    "    '0': 'Modified Curriculum',\n",
    "    '1': 'Abridged/Adapted (Remedial)',\n",
    "    '2': 'Standard Version',\n",
    "    '5': 'Honors/Advanced/Academically Gifted',\n",
    "    '6': 'Co-op Education',\n",
    "    '7': 'Advanced Placement',\n",
    "    '8': 'International Baccalaureate',\n",
    "    '9': 'Non-Classroom Activity'\n",
    "}\n",
    "\n",
    "def convert_to_description(val):\n",
    "    val = str(val).strip()\n",
    "    return conversion_map.get(val[0], val) if val else val\n",
    "\n",
    "# Apply to your column (replace 'your_column' with the actual name)\n",
    "transcripts['academic_level_desc'] = transcripts['academic_level_desc'].apply(convert_to_description)\n",
    "\n",
    "transcripts = transcripts[transcripts['include_in_gpa'] == 'Y']\n",
    "\n",
    "letter_to_score = {\n",
    "    'A+': 98,\n",
    "    'A': 95,\n",
    "    'A-': 91,\n",
    "    'B+': 88,\n",
    "    'B': 85,\n",
    "    'B-': 81,\n",
    "    'C+': 78,\n",
    "    'C': 75,\n",
    "    'C-': 71,\n",
    "    'D+': 68,\n",
    "    'D': 65,\n",
    "    'D-': 61,\n",
    "    'F': 50,\n",
    "    'P': None,   # Pass/fail — handle as missing or special flag\n",
    "    'W': None,   # Withdrawn — usually not a valid mark\n",
    "    'INC': None, # Incomplete\n",
    "    'EX': None   # Exempt\n",
    "}\n",
    "\n",
    "def convert_final_mark(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Try converting to float (for numeric values)\n",
    "        num = float(val)\n",
    "        if 0 <= num <= 100:\n",
    "            return num\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Handle letter grades\n",
    "    val_str = str(val).strip().upper()\n",
    "    return letter_to_score.get(val_str, None)\n",
    "\n",
    "transcripts['final_mark'] = transcripts['final_mark'].apply(convert_final_mark)\n",
    "\n",
    "transcripts.dropna(subset=['final_mark'])\n",
    "max_classes = 50\n",
    "\n",
    "# Sort and rank\n",
    "transcripts_sorted = transcripts.sort_values(by=['mastid', 'grade', 'academic_level_desc'])\n",
    "transcripts_sorted['class_rank'] = transcripts_sorted.groupby('mastid').cumcount() + 1\n",
    "\n",
    "# Trim long histories\n",
    "transcripts_trimmed = transcripts_sorted[transcripts_sorted['class_rank'] <= max_classes]\n",
    "\n",
    "# Pivot helper\n",
    "def pivot_feature(df, colname):\n",
    "    out = df.pivot(index='mastid', columns='class_rank', values=colname)\n",
    "    out.columns = [f\"{colname}_{i}\" for i in out.columns]\n",
    "    return out\n",
    "\n",
    "# Pivot all desired features\n",
    "features = {\n",
    "    'course_desc': pivot_feature(transcripts_trimmed, 'course_desc'),\n",
    "    'grade': pivot_feature(transcripts_trimmed, 'grade'),\n",
    "    'academic_level_desc': pivot_feature(transcripts_trimmed, 'academic_level_desc'),\n",
    "    'final_mark': pivot_feature(transcripts_trimmed, 'final_mark')\n",
    "}\n",
    "\n",
    "# Base student info — from the full transcripts, not trimmed\n",
    "student_info = transcripts.groupby('mastid')[['lea', 'schlcode']].first()\n",
    "\n",
    "# Merge all\n",
    "student_df = student_info.copy()\n",
    "for df in features.values():\n",
    "    student_df = student_df.merge(df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Reset index for ML-ready DataFrame\n",
    "student_df = student_df.reset_index()\n",
    "\n",
    "student_df.to_csv(\"../data/transcripts_pivoted.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
