{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/Users/danielmatten/Desktop/m/transcripts20\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.sas7bdat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Read SAS file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Keep only relevant columns\u001b[39;00m\n\u001b[32m     14\u001b[39m     df.columns = df.columns.str.lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/sas/sasreader.py:178\u001b[39m, in \u001b[36mread_sas\u001b[39m\u001b[34m(filepath_or_buffer, format, index, encoding, chunksize, iterator, compression)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reader\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m reader:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/sas/sas7bdat.py:685\u001b[39m, in \u001b[36mSAS7BDATReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28mself\u001b[39m._current_row_in_chunk_index = \u001b[32m0\u001b[39m\n\u001b[32m    684\u001b[39m p = Parser(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m rslt = \u001b[38;5;28mself\u001b[39m._chunk_to_dataframe()\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32msas.pyx:382\u001b[39m, in \u001b[36mpandas._libs.sas.Parser.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msas.pyx:473\u001b[39m, in \u001b[36mpandas._libs.sas.Parser.readline\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msas.pyx:394\u001b[39m, in \u001b[36mpandas._libs.sas.Parser.read_next_page\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/io/sas/sas7bdat.py:693\u001b[39m, in \u001b[36mSAS7BDATReader._read_next_page\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    689\u001b[39m         rslt = rslt.set_index(\u001b[38;5;28mself\u001b[39m.index)\n\u001b[32m    691\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rslt\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_next_page\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    694\u001b[39m     \u001b[38;5;28mself\u001b[39m._current_page_data_subheader_pointers = []\n\u001b[32m    695\u001b[39m     \u001b[38;5;28mself\u001b[39m._cached_page = \u001b[38;5;28mself\u001b[39m._path_or_buf.read(\u001b[38;5;28mself\u001b[39m._page_length)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "transcripts = pd.DataFrame()\n",
    "for year in range(18, 24):  # range is exclusive at the end\n",
    "    filename = f\"/Users/danielmatten/Desktop/m/transcripts20{year}.sas7bdat\"\n",
    "    try:\n",
    "        # Read SAS file\n",
    "        df = pd.read_sas(filename)\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # Drop missing values\n",
    "\n",
    "        \n",
    "        # Append to the main DataFrame\n",
    "        transcripts = pd.concat([transcripts, df], ignore_index=True)\n",
    "        print(f\"{year} done\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping.\")\n",
    "    except KeyError:\n",
    "        print(f\"Required columns not found in {filename}. Skipping.\")\n",
    "def clean_bytes(val):\n",
    "    if isinstance(val, bytes):\n",
    "        return str(val)[2:-1]  # str(b'xyz') => \"b'xyz'\" → \"xyz\"\n",
    "    return val\n",
    "# Drop unnecessary columns\n",
    "transcripts.drop(columns=['semester', 'how_taken_desc', 'course_length_desc','school','completed_date','course_code'], inplace=True)\n",
    "\n",
    "# Drop rows where 'mastid' is NaN\n",
    "#transcripts.dropna(subset=['mastid'], inplace=True)\n",
    "print(\"nas\")\n",
    "# Decode bytes to string in all columns (no slicing needed)\n",
    "for col in transcripts.columns:\n",
    "    transcripts[col] = transcripts[col].apply(clean_bytes)\n",
    "    print(col)\n",
    "\n",
    "transcripts.dropna(inplace=True) #this is pretty safe\n",
    "\n",
    "transcripts.to_csv(\"../data/transcripts_master.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping based on the examples you gave\n",
    "conversion_map = {\n",
    "    '0': 'Modified Curriculum',\n",
    "    '1': 'Abridged/Adapted (Remedial)',\n",
    "    '2': 'Standard Version',\n",
    "    '5': 'Honors/Advanced/Academically Gifted',\n",
    "    '6': 'Co-op Education',\n",
    "    '7': 'Advanced Placement',\n",
    "    '8': 'International Baccalaureate',\n",
    "    '9': 'Non-Classroom Activity'\n",
    "}\n",
    "\n",
    "def convert_to_description(val):\n",
    "    val = str(val).strip()\n",
    "    return conversion_map.get(val[0], val) if val else val\n",
    "\n",
    "# Apply to your column (replace 'your_column' with the actual name)\n",
    "transcripts['academic_level_desc'] = transcripts['academic_level_desc'].apply(convert_to_description)\n",
    "\n",
    "transcripts = transcripts[transcripts['include_in_gpa'] == 'Y']\n",
    "\n",
    "letter_to_score = {\n",
    "    'A+': 98,\n",
    "    'A': 95,\n",
    "    'A-': 91,\n",
    "    'B+': 88,\n",
    "    'B': 85,\n",
    "    'B-': 81,\n",
    "    'C+': 78,\n",
    "    'C': 75,\n",
    "    'C-': 71,\n",
    "    'D+': 68,\n",
    "    'D': 65,\n",
    "    'D-': 61,\n",
    "    'F': 50,\n",
    "    'P': None,   # Pass/fail — handle as missing or special flag\n",
    "    'W': None,   # Withdrawn — usually not a valid mark\n",
    "    'INC': None, # Incomplete\n",
    "    'EX': None   # Exempt\n",
    "}\n",
    "\n",
    "def convert_final_mark(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Try converting to float (for numeric values)\n",
    "        num = float(val)\n",
    "        if 0 <= num <= 100:\n",
    "            return num\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Handle letter grades\n",
    "    val_str = str(val).strip().upper()\n",
    "    return letter_to_score.get(val_str, None)\n",
    "\n",
    "transcripts['final_mark'] = transcripts['final_mark'].apply(convert_final_mark)\n",
    "\n",
    "transcripts.dropna(subset=['final_mark'])\n",
    "max_classes = 50\n",
    "\n",
    "# Sort and rank\n",
    "transcripts_sorted = transcripts.sort_values(by=['mastid', 'grade', 'academic_level_desc'])\n",
    "transcripts_sorted['class_rank'] = transcripts_sorted.groupby('mastid').cumcount() + 1\n",
    "\n",
    "# Trim long histories\n",
    "transcripts_trimmed = transcripts_sorted[transcripts_sorted['class_rank'] <= max_classes]\n",
    "\n",
    "# Pivot helper\n",
    "def pivot_feature(df, colname):\n",
    "    out = df.pivot(index='mastid', columns='class_rank', values=colname)\n",
    "    out.columns = [f\"{colname}_{i}\" for i in out.columns]\n",
    "    return out\n",
    "\n",
    "# Pivot all desired features\n",
    "features = {\n",
    "    'course_desc': pivot_feature(transcripts_trimmed, 'course_desc'),\n",
    "    'grade': pivot_feature(transcripts_trimmed, 'grade'),\n",
    "    'academic_level_desc': pivot_feature(transcripts_trimmed, 'academic_level_desc'),\n",
    "    'final_mark': pivot_feature(transcripts_trimmed, 'final_mark')\n",
    "}\n",
    "\n",
    "# Base student info — from the full transcripts, not trimmed\n",
    "student_info = transcripts.groupby('mastid')[['lea', 'schlcode']].first()\n",
    "\n",
    "# Merge all\n",
    "student_df = student_info.copy()\n",
    "for df in features.values():\n",
    "    student_df = student_df.merge(df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Reset index for ML-ready DataFrame\n",
    "student_df = student_df.reset_index()\n",
    "\n",
    "student_df.to_csv(\"../data/transcripts_pivoted.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
